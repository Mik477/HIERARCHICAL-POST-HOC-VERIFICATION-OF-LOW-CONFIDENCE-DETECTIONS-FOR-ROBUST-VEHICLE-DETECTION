{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e32fa6",
   "metadata": {},
   "source": [
    "# Hierarchical Inference Pipeline\n",
    "\n",
    "This notebook demonstrates the complete inference pipeline for our Hierarchical Object Detector. It uses a **waterfall rescue model** to enhance the predictions of a baseline YOLO L3 model.\n",
    "\n",
    "### Key Features of this Pipeline:\n",
    "1.  **Waterfall Rescue Logic**:\n",
    "    -   High-confidence L3 detections (`>0.5`) are automatically confirmed.\n",
    "    -   Low-confidence L3 candidates (`0.1-0.5`) are passed to the L2 head for a rescue attempt.\n",
    "    -   If L2 fails, the L1 head gets a final chance to rescue the detection as a generic \"vehicle\".\n",
    "2.  **`HierarchicalDetector` Class**: A single, powerful class that encapsulates all models and logic.\n",
    "3.  **Run-Once Efficiency**: The expensive YOLO backbone is run only **once** per image.\n",
    "4.  **High-Quality Visualization**: Produces clear, color-coded bounding boxes, with an option to show or hide text labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba1e751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Core Imports ---\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda:0\" if USE_GPU else \"cpu\")\n",
    "\n",
    "# --- PATHS (Update these to match your project) ---\n",
    "YOLO_L3_PATH = Path(\"models/yolov11_baseline.pt\")\n",
    "AUX_HEADS_PATH = Path(\"models/hierarchical_heads.pt\")\n",
    "MANIFEST_PATH = Path(\"manifest.json\")\n",
    "\n",
    "# --- FOLDERS ---\n",
    "IMAGE_INPUT_DIR = Path(\"C:/Users/Mika/Desktop/New_Training_Run_Post_Bulgaria/mio_tcd_yolo_vehicles_only/images/val\") # <--- UPDATE THIS\n",
    "OUTPUT_DIR = Path(\"inference_results_waterfall\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# --- WATERFALL INFERENCE THRESHOLDS (Tune these to adjust performance) ---\n",
    "L3_CONFIRM_THRESH = 0.5\n",
    "L3_CANDIDATE_THRESH = 0.1\n",
    "L2_RESCUE_THRESH = 0.6\n",
    "L1_RESCUE_THRESH = 0.5\n",
    "\n",
    "# --- Visualization Settings ---\n",
    "FONT_PATH = \"C:/Windows/Fonts/bahnschrift.ttf\"\n",
    "NUM_IMAGES_TO_PROCESS = 50\n",
    "# --- NEW: Set this to False to only draw boxes without text labels ---\n",
    "DRAW_LABELS = True\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "if not IMAGE_INPUT_DIR.exists():\n",
    "    print(f\"[ERROR] The specified image input directory does not exist: {IMAGE_INPUT_DIR}\")\n",
    "    print(\"Please update the `IMAGE_INPUT_DIR` variable in this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd63b6",
   "metadata": {},
   "source": [
    "## The `HierarchicalDetector` Class\n",
    "\n",
    "This class is the core engine of our inference pipeline. It has been updated to use the **waterfall rescue model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e4c9948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Models (Must match training definitions) ---\n",
    "class FeatureCompressor(nn.Module):\n",
    "    # This class is unchanged, but included for completeness of the cell.\n",
    "    def __init__(self, in_channels: int, out_dim: int, hidden_dim_factor: int = 2):\n",
    "        super().__init__()\n",
    "        hidden_dim = out_dim * hidden_dim_factor\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(hidden_dim), nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, out_dim, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(out_dim), nn.SiLU(inplace=True),\n",
    "        ); self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: return torch.flatten(self.pool(self.trunk(x)), 1)\n",
    "\n",
    "# --- REVISED Model Definition ---\n",
    "class AuxHeadsMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    An MLP with a shared trunk and DECOUPLED necks for the L1 and L2 heads.\n",
    "    This architecture must match the one used in the training script.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, num_l2_classes: int, hidden_dim: int = 512, neck_dim: int = 128, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        # 1. Shared Trunk\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(inplace=True), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), nn.LayerNorm(hidden_dim // 2), nn.SiLU(inplace=True), nn.Dropout(dropout)\n",
    "        )\n",
    "        # --- ARCHITECTURAL CHANGE: Decoupled Necks ---\n",
    "        # 2a. L2 Neck\n",
    "        self.l2_neck = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, neck_dim), nn.LayerNorm(neck_dim), nn.SiLU(inplace=True)\n",
    "        )\n",
    "        # 2b. L1 Neck\n",
    "        self.l1_neck = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, neck_dim), nn.LayerNorm(neck_dim), nn.SiLU(inplace=True)\n",
    "        )\n",
    "        # 3. Final Heads\n",
    "        self.l2_head = nn.Linear(neck_dim, num_l2_classes)\n",
    "        self.l1_head = nn.Linear(neck_dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        shared_features = self.trunk(x)\n",
    "        l2_features = self.l2_neck(shared_features)\n",
    "        l1_features = self.l1_neck(shared_features)\n",
    "        return {\"l1_logits\": self.l1_head(l1_features), \"l2_logits\": self.l2_head(l2_features)}\n",
    "\n",
    "# --- Main Inference Class (Unchanged, but included for completeness) ---\n",
    "class HierarchicalDetector:\n",
    "    def __init__(self, yolo_path: Path, aux_heads_path: Path, manifest_path: Path):\n",
    "        print(\"Initializing HierarchicalDetector...\"); self.device = DEVICE\n",
    "        with open(manifest_path) as f: self.manifest = json.load(f)\n",
    "        self.hierarchy = self.manifest['hierarchy']; self.l3_names = self.hierarchy['L3_NAMES']; self.l2_names = self.hierarchy['L2_NAMES']; self.class_to_l2 = self.hierarchy['CLASS_TO_L2']\n",
    "        feat_config = self.manifest['feature_extraction']; self.compressed_dim = feat_config['compressed_dim']; self.roi_align_size = tuple(feat_config['roi_align_size']); self.pyramid_thresholds = tuple(feat_config['pyramid_thresholds'])\n",
    "        self.yolo_model = YOLO(yolo_path); self.feature_maps = {}; self._attach_hooks()\n",
    "        self._instantiate_aux_models()\n",
    "        checkpoint = torch.load(aux_heads_path, map_location=self.device)\n",
    "        self.aux_heads_mlp.load_state_dict(checkpoint['model_state_dict']); self.aux_heads_mlp.eval(); self.feature_compressors.eval()\n",
    "        print(\"HierarchicalDetector initialized and ready.\")\n",
    "\n",
    "    def _attach_hooks(self):\n",
    "        def get_features_hook(name):\n",
    "            def hook(model, input, output): self.feature_maps[name] = output[0] if isinstance(output, tuple) else output\n",
    "            return hook\n",
    "        try:\n",
    "            # NOTE: YOLOv11 neck indices are different from YOLOv8. These are for v11.\n",
    "            # P5 is output of layer 16, P4 is layer 19, P3 is layer 22.\n",
    "            p5_module_idx, p4_module_idx, p3_module_idx = 16, 19, 22\n",
    "            self.hook_p5 = self.yolo_model.model.model[p5_module_idx].register_forward_hook(get_features_hook('p5'))\n",
    "            self.hook_p4 = self.yolo_model.model.model[p4_module_idx].register_forward_hook(get_features_hook('p4'))\n",
    "            self.hook_p3 = self.yolo_model.model.model[p3_module_idx].register_forward_hook(get_features_hook('p3'))\n",
    "            print(f\"Attached forward hooks to layers {p3_module_idx}(P3), {p4_module_idx}(P4), {p5_module_idx}(P5).\")\n",
    "        except Exception as e: print(f\"Hook attachment failed: {e}. Please verify model architecture and indices.\"); raise e\n",
    "        # Run a dummy forward pass to populate strides and feature maps for the first time\n",
    "        self.yolo_model.predict(torch.zeros(1, 3, 640, 640).to(self.device), verbose=False)\n",
    "        self.strides = {'p3': self.yolo_model.model.stride[0], 'p4': self.yolo_model.model.stride[1], 'p5': self.yolo_model.model.stride[2]}\n",
    "\n",
    "    def _instantiate_aux_models(self):\n",
    "        # A dummy forward pass is needed to get the channel dimensions from the hooks\n",
    "        self.yolo_model.predict(torch.zeros(1, 3, 640, 640).to(self.device), verbose=False)\n",
    "        p3_channels, p4_channels, p5_channels = self.feature_maps['p3'].shape[1], self.feature_maps['p4'].shape[1], self.feature_maps['p5'].shape[1]\n",
    "        self.feature_compressors = nn.ModuleDict({'p3': FeatureCompressor(p3_channels, self.compressed_dim), 'p4': FeatureCompressor(p4_channels, self.compressed_dim), 'p5': FeatureCompressor(p5_channels, self.compressed_dim)}).to(self.device)\n",
    "        self.aux_heads_mlp = AuxHeadsMLP(self.compressed_dim, len(self.l2_names)).to(self.device)\n",
    "\n",
    "    def _extract_compact_features(self, candidate_boxes: torch.Tensor) -> torch.Tensor:\n",
    "        from torchvision.ops import roi_align\n",
    "        def choose_pyramid_level(boxes_xyxy: torch.Tensor, thresholds: Tuple[float, float]):\n",
    "            if boxes_xyxy.numel() == 0: return torch.empty(0, dtype=torch.long, device=boxes_xyxy.device)\n",
    "            max_side = torch.maximum(boxes_xyxy[:, 2] - boxes_xyxy[:, 0], boxes_xyxy[:, 3] - boxes_xyxy[:, 1])\n",
    "            levels = torch.full_like(max_side, 4, dtype=torch.long); levels[max_side <= thresholds[0]] = 3; levels[max_side > thresholds[1]] = 5\n",
    "            return levels\n",
    "        levels = choose_pyramid_level(candidate_boxes, self.pyramid_thresholds)\n",
    "        final_features = torch.zeros(len(candidate_boxes), self.compressed_dim, device=self.device)\n",
    "        for level_idx in [3, 4, 5]:\n",
    "            mask = (levels == level_idx)\n",
    "            if not mask.any(): continue\n",
    "            level_name = f'p{level_idx}'; boxes_on_level = candidate_boxes[mask]\n",
    "            # roi_align expects a list of tensors, one for each image in the batch. We have a batch size of 1.\n",
    "            pooled_feats = roi_align(self.feature_maps[level_name], [boxes_on_level], output_size=self.roi_align_size, spatial_scale=1.0 / self.strides[level_name])\n",
    "            if pooled_feats.numel() > 0: final_features[mask] = self.feature_compressors[level_name](pooled_feats)\n",
    "        return final_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, image_path: str, l3_confirm_thresh: float, l3_candidate_thresh: float, l2_rescue_thresh: float, l1_rescue_thresh: float, verbose: bool = False):\n",
    "        # This entire method is unchanged.\n",
    "        results = self.yolo_model(image_path, conf=l3_candidate_thresh, device=self.device, verbose=False)\n",
    "        result = results[0]\n",
    "        if result.boxes is None or len(result.boxes) == 0: return [], result\n",
    "        \n",
    "        candidates = result.boxes; candidate_boxes = candidates.xyxy\n",
    "        compact_features = self._extract_compact_features(candidate_boxes)\n",
    "        aux_outputs = self.aux_heads_mlp(compact_features)\n",
    "        l1_probs = torch.sigmoid(aux_outputs['l1_logits']).squeeze(-1)\n",
    "        l2_probs, l2_indices = torch.max(torch.softmax(aux_outputs['l2_logits'], dim=1), dim=1)\n",
    "        \n",
    "        final_detections = []\n",
    "        if verbose: print(\"\\n--- Processing Candidates ---\")\n",
    "        for i in range(len(candidates)):\n",
    "            l3_conf, l3_cls_idx = candidates.conf[i].item(), candidates.cls[i].int().item()\n",
    "            l3_cls_name = self.l3_names[l3_cls_idx]\n",
    "            \n",
    "            if l3_conf >= l3_confirm_thresh:\n",
    "                final_detections.append({'bbox': candidates.xyxy[i].cpu().numpy(), 'label': l3_cls_name, 'confidence': l3_conf, 'type': 'confirmed'})\n",
    "                continue\n",
    "\n",
    "            l1_prob, l2_prob, l2_idx = l1_probs[i].item(), l2_probs[i].item(), l2_indices[i].item()\n",
    "            l2_name = self.l2_names[l2_idx]\n",
    "            is_consistent = self.class_to_l2.get(l3_cls_name) == l2_name\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Candidate {i} (L3: {l3_cls_name} @ {l3_conf:.2f}):\")\n",
    "                print(f\"    - L2 Score: {l2_prob:.2f} ({l2_name}, consistent: {is_consistent})\")\n",
    "                print(f\"    - L1 Score: {l1_prob:.2f}\")\n",
    "\n",
    "            if l2_prob >= l2_rescue_thresh and is_consistent:\n",
    "                if verbose: print(\"    - RESCUED by L2\")\n",
    "                final_detections.append({'bbox': candidates.xyxy[i].cpu().numpy(), 'label': l2_name, 'confidence': l2_prob, 'type': 'rescued_l2'})\n",
    "                continue\n",
    "            \n",
    "            if l1_prob >= l1_rescue_thresh:\n",
    "                if verbose: print(\"    - RESCUED by L1\")\n",
    "                final_detections.append({'bbox': candidates.xyxy[i].cpu().numpy(), 'label': \"vehicle\", 'confidence': l1_prob, 'type': 'rescued_l1'})\n",
    "                continue\n",
    "            \n",
    "            if verbose: print(\"    - REJECTED\")\n",
    "            \n",
    "        return final_detections, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7608f93",
   "metadata": {},
   "source": [
    "## Hierarchical NMS & Visualization Functions\n",
    "\n",
    "This cell contains our key post-processing and visualization logic.\n",
    "\n",
    "### Hierarchical Non-Maximum Suppression (NMS)\n",
    "To solve the problem of multiple overlapping boxes for a single object, we implement a custom NMS function. It sorts detections not just by confidence, but by a **priority score** based on the waterfall logic (`confirmed` > `rescued_l2` > `rescued_l1`). This ensures the most specific and reliable detection is always chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19807273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using font: Bahnschrift\n",
      "Hierarchical NMS and Visualization helpers defined.\n"
     ]
    }
   ],
   "source": [
    "# --- NMS and Visualization Helpers ---\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculates Intersection over Union for two bounding boxes.\"\"\"\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
    "    if inter_area == 0: return 0.0\n",
    "    \n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box1[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    return inter_area / union_area\n",
    "\n",
    "def hierarchical_nms(detections: List[Dict], iou_threshold: float) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Performs Non-Maximum Suppression, prioritizing detections based on the waterfall logic.\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "\n",
    "    priority_map = {'confirmed': 3, 'rescued_l2': 2, 'rescued_l1': 1}\n",
    "    sorted_detections = sorted(\n",
    "        detections,\n",
    "        key=lambda d: (priority_map.get(d['type'], 0), d['confidence']),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    kept_detections = []\n",
    "    while sorted_detections:\n",
    "        best_det = sorted_detections.pop(0)\n",
    "        kept_detections.append(best_det)\n",
    "        remaining_detections = []\n",
    "        for det in sorted_detections:\n",
    "            if calculate_iou(best_det['bbox'], det['bbox']) < iou_threshold:\n",
    "                remaining_detections.append(det)\n",
    "        sorted_detections = remaining_detections\n",
    "        \n",
    "    return kept_detections\n",
    "\n",
    "# --- NEW: Advanced Visualization Code with Collision Avoidance ---\n",
    "try:\n",
    "    from matplotlib.font_manager import FontProperties\n",
    "    font_prop = FontProperties(fname=FONT_PATH, size=11)\n",
    "    FONT_NAME = font_prop.get_name()\n",
    "    print(f\"Using font: {FONT_NAME}\")\n",
    "except Exception:\n",
    "    print(f\"Font at {FONT_PATH} not found. Using default 'sans-serif'.\")\n",
    "    FONT_NAME = 'sans-serif'\n",
    "    font_prop = FontProperties(family=FONT_NAME, size=11, weight='bold')\n",
    "\n",
    "plt.rcParams['font.family'] = FONT_NAME\n",
    "\n",
    "COLORS = {\n",
    "    'baseline_high_conf': '#3498db', # Blue\n",
    "    'confirmed': '#2ecc71',          # Green\n",
    "    'rescued_l2': '#e67e22',          # Orange\n",
    "    'rescued_l1': '#e74c3c',          # Red\n",
    "}\n",
    "\n",
    "def _get_text_bbox(ax, x, y, text, font_prop):\n",
    "    \"\"\"\n",
    "    Internal helper to calculate the bounding box of a text label in data coordinates\n",
    "    before it is drawn. This is essential for collision detection.\n",
    "    \"\"\"\n",
    "    text_obj = ax.text(x, y, text, fontproperties=font_prop, \n",
    "                       bbox=dict(facecolor='red', alpha=1, pad=3, edgecolor='none', boxstyle='round,pad=0.4'))\n",
    "    # Use the figure's renderer to calculate the pixel bounding box\n",
    "    renderer = ax.get_figure().canvas.get_renderer()\n",
    "    bbox_pixel = text_obj.get_window_extent(renderer=renderer)\n",
    "    # Transform the pixel bounding box back into data coordinates\n",
    "    bbox_data = ax.transData.inverted().transform(bbox_pixel)\n",
    "    text_obj.remove() # Clean up the temporary text object\n",
    "    return bbox_data\n",
    "\n",
    "def _check_overlap(box1, box2):\n",
    "    \"\"\"Checks if two bounding boxes (x1, y1, x2, y2) overlap.\"\"\"\n",
    "    return not (box1[2] < box2[0] or box1[0] > box2[2] or box1[3] < box2[1] or box1[1] > box2[3])\n",
    "\n",
    "def draw_detections_with_labels(ax, detections, color_map_key, draw_labels=True):\n",
    "    \"\"\"\n",
    "    Main function to draw detection boxes and their labels with intelligent\n",
    "    placement and collision avoidance.\n",
    "    \"\"\"\n",
    "    drawn_label_bboxes = []\n",
    "\n",
    "    for det in detections:\n",
    "        # Draw the primary detection bounding box\n",
    "        bbox = det['bbox']\n",
    "        color = COLORS[det.get('type', color_map_key)]\n",
    "        ax.add_patch(patches.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1], \n",
    "                                       linewidth=2.5, edgecolor=color, facecolor='none', alpha=0.9))\n",
    "\n",
    "        if not draw_labels:\n",
    "            continue\n",
    "            \n",
    "        # --- Label Placement Logic ---\n",
    "        label_text = det['label']\n",
    "        \n",
    "        # 1. Calculate the initial desired position and size of the label\n",
    "        # Get the text bounding box to know its height for boundary checks\n",
    "        text_bbox_dims = _get_text_bbox(ax, 0, 0, label_text, font_prop)\n",
    "        text_height = text_bbox_dims[1][1] - text_bbox_dims[0][1]\n",
    "\n",
    "        # Initial position: top-left corner of the detection box\n",
    "        x, y = bbox[0], bbox[1]\n",
    "        \n",
    "        # Boundary check: If the label would go off-screen at the top, move to bottom-left\n",
    "        if y < text_height:\n",
    "            y = bbox[3]\n",
    "\n",
    "        # 2. De-confliction Loop: Check for overlaps and shift if necessary\n",
    "        is_colliding = True\n",
    "        while is_colliding:\n",
    "            is_colliding = False\n",
    "            # Calculate the bounding box of the label at its current candidate position (x, y)\n",
    "            current_label_bbox = _get_text_bbox(ax, x, y, label_text, font_prop)\n",
    "            \n",
    "            for drawn_box in drawn_label_bboxes:\n",
    "                if _check_overlap(current_label_bbox, drawn_box):\n",
    "                    is_colliding = True\n",
    "                    # If collision, shift the label down by its height and re-check\n",
    "                    y += text_height * 1.1 \n",
    "                    break\n",
    "        \n",
    "        # 3. Draw the final, non-overlapping label\n",
    "        ax.text(x, y, label_text, fontproperties=font_prop, color='white',\n",
    "                verticalalignment='top', horizontalalignment='left',\n",
    "                bbox=dict(facecolor=color, alpha=0.8, pad=3, edgecolor='none', boxstyle='round,pad=0.4'))\n",
    "        \n",
    "        # 4. Add the final label's bounding box to our list of drawn labels\n",
    "        final_label_bbox = _get_text_bbox(ax, x, y, label_text, font_prop)\n",
    "        drawn_label_bboxes.append(final_label_bbox)\n",
    "\n",
    "print(\"Hierarchical NMS and Advanced Visualization helpers defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84a686",
   "metadata": {},
   "source": [
    "## 🚀 Cell 4: Main Inference Loop\n",
    "\n",
    "This is the main execution cell. It performs the following steps:\n",
    "1.  Instantiates our `HierarchicalDetector`.\n",
    "2.  Selects a random sample of images from the specified input directory.\n",
    "3.  For each image, it gets the **raw hierarchical detections**.\n",
    "4.  It then runs our **`hierarchical_nms`** function to clean up overlapping boxes.\n",
    "5.  Finally, it generates and displays the side-by-side comparison plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NMS Threshold ---\n",
    "NMS_IOU_THRESH = 0.5 # IoU threshold for suppressing overlapping boxes\n",
    "\n",
    "# --- Instantiate the Detector ---\n",
    "try:\n",
    "    detector = HierarchicalDetector(YOLO_L3_PATH, AUX_HEADS_PATH, MANIFEST_PATH)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n[FATAL ERROR] Could not initialize detector. A required file was not found: {e}\")\n",
    "    print(\"Please check the paths in Cell 1.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FATAL ERROR] An unexpected error occurred during initialization: {e}\")\n",
    "\n",
    "# --- Get Images and Run Inference ---\n",
    "if 'detector' in locals():\n",
    "    image_files = [p for p in IMAGE_INPUT_DIR.glob(\"**/*\") if p.suffix.lower() in ('.jpg', '.jpeg', '.png')]\n",
    "    if not image_files:\n",
    "        print(f\"[ERROR] No images found in {IMAGE_INPUT_DIR}. Please check the path.\")\n",
    "    else:\n",
    "        sample_images = random.sample(image_files, k=min(NUM_IMAGES_TO_PROCESS, len(image_files)))\n",
    "        print(f\"\\nStarting inference on {len(sample_images)} images...\")\n",
    "\n",
    "        for i, img_path in enumerate(tqdm(sample_images, desc=\"Processing images\")):\n",
    "            # --- Run Inference to get raw detections ---\n",
    "            raw_hierarchical_detections, baseline_result = detector.predict(\n",
    "                image_path=str(img_path),\n",
    "                l3_confirm_thresh=L3_CONFIRM_THRESH,\n",
    "                l3_candidate_thresh=L3_CANDIDATE_THRESH,\n",
    "                l2_rescue_thresh=L2_RESCUE_THRESH,\n",
    "                l1_rescue_thresh=L1_RESCUE_THRESH,\n",
    "                verbose=False # Set to True for detailed per-candidate logs\n",
    "            )\n",
    "            \n",
    "            # --- Apply Hierarchical NMS ---\n",
    "            final_detections = hierarchical_nms(raw_hierarchical_detections, NMS_IOU_THRESH)\n",
    "            \n",
    "            # --- Plotting ---\n",
    "            img_rgb = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB)\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(28, 14))\n",
    "            fig.patch.set_facecolor('white')\n",
    "\n",
    "            # --- Plot Baseline Results ---\n",
    "            axs[0].imshow(img_rgb)\n",
    "            axs[0].set_title(f\"Baseline YOLO (conf > {L3_CONFIRM_THRESH})\", fontsize=18, weight='bold', pad=20)\n",
    "            axs[0].axis('off')\n",
    "            \n",
    "            baseline_detections_to_draw = []\n",
    "            if baseline_result.boxes is not None:\n",
    "                for box in baseline_result.boxes:\n",
    "                    if box.conf.item() >= L3_CONFIRM_THRESH:\n",
    "                        label_name = detector.l3_names[int(box.cls.item())]\n",
    "                        label = f\"{label_name.upper()}: {box.conf.item():.2f}\"\n",
    "                        baseline_detections_to_draw.append({\n",
    "                            'bbox': box.xyxy[0].cpu().numpy(),\n",
    "                            'label': label,\n",
    "                            'type': 'baseline_high_conf' # Key for color mapping\n",
    "                        })\n",
    "            draw_detections_with_labels(axs[0], baseline_detections_to_draw, 'baseline_high_conf', draw_labels=DRAW_LABELS)\n",
    "\n",
    "            # --- Plot Hierarchical Results (after NMS) ---\n",
    "            axs[1].imshow(img_rgb)\n",
    "            axs[1].set_title(\"Hierarchical Model (after Waterfall Rescue + NMS)\", fontsize=18, weight='bold', pad=20)\n",
    "            axs[1].axis('off')\n",
    "            \n",
    "            # Prepare labels for hierarchical detections\n",
    "            hierarchical_detections_to_draw = []\n",
    "            for det in final_detections:\n",
    "                label_text = det['label'].replace('_', ' ').upper()\n",
    "                conf = det['confidence']\n",
    "                det_type = det['type']\n",
    "                if det_type == 'confirmed': label = f\"{label_text}: {conf:.2f}\"\n",
    "                elif det_type == 'rescued_l2': label = f\"{label_text} [L2 RESCUED]: {conf:.2f}\"\n",
    "                elif det_type == 'rescued_l1': label = f\"{label_text.upper()} [L1 RESCUED]: {conf:.2f}\"\n",
    "                det_copy = det.copy()\n",
    "                det_copy['label'] = label\n",
    "                hierarchical_detections_to_draw.append(det_copy)\n",
    "            draw_detections_with_labels(axs[1], hierarchical_detections_to_draw, 'confirmed', draw_labels=DRAW_LABELS)\n",
    "            \n",
    "            plt.tight_layout(pad=1.5)\n",
    "            save_path = OUTPUT_DIR / f\"comparison_{img_path.stem}.png\"\n",
    "            plt.savefig(save_path, dpi=120, bbox_inches='tight')\n",
    "            # print(f\"Saved comparison image to: {save_path}\") # Optional: uncomment for verbose saving\n",
    "            \n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "\n",
    "        print(f\"\\n✅ Inference complete. {len(sample_images)} comparison images saved to '{OUTPUT_DIR}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hier-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
