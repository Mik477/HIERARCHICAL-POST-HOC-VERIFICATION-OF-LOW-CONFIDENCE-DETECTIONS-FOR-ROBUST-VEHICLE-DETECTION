{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16168c7a",
   "metadata": {},
   "source": [
    "# Hierarchical Aux-Heads Dataset Builder (from L3 outputs)\n",
    "\n",
    "This notebook prepares **chunked ROI files** to train **auxiliary L2/L1 heads** (groups/super) *off the base L3 YOLO model*.\n",
    "\n",
    "**Key features**\n",
    "- Runs L3 detector on the train split to harvest:\n",
    "  - **Predicted positives** (TPs by IoU≥0.5),\n",
    "  - **Predicted false positives** (FPs by IoU≤0.3),\n",
    "  - **Ambiguous** predictions (0.3<IoU<0.5) → **ignored**,\n",
    "- Adds **simulated detections**:\n",
    "  - **Jittered positives** (IoU≥0.5) to **build tolerance** to minor misalignment,\n",
    "  - **Jittered negatives** (IoU≤0.3),\n",
    "  - **Background negatives** (random boxes with IoU<0.05 to any GT),\n",
    "- Saves **chunked JSONL.GZ** files with ROI metadata + labels for L2/L1,\n",
    "- Produces **class weights** (effective-number) for L2,\n",
    "- Writes a **manifest.json** for the training script.\n",
    "\n",
    "**Why IoU-conditioned labeling?**\n",
    "- We *accept* slight jitter (IoU≥0.5) as positives so aux heads don’t kill good-but-slightly-off detections,\n",
    "- We *reject* clear mistakes (IoU≤0.3) as negatives to build resilience,\n",
    "- We **ignore** in-between to avoid noisy supervision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267d93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os, json, gzip, math, random\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897f876",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "This cell centralizes all parameters for the data preparation pipeline. We have introduced several new parameters to support the **Unified-Compact-ROI** feature extraction strategy.\n",
    "\n",
    "### Key Engineering Decisions:\n",
    "- **Output Format**: We are now generating `.pt` (PyTorch tensor) files instead of `jsonl.gz`. This is vastly more efficient for storing and loading large tensors.\n",
    "- **Feature Extraction (`ROI_` & `COMPRESSED_` params)**: We will perform `roi_align` on a single, scale-appropriate feature map for each box. The `PYRAMID_THRESHOLDS` determine which map to use. The resulting `7x7` feature patch is immediately compressed to a small `256-dim` vector before being saved, solving the I/O bottleneck.\n",
    "- **Batching (`PREP_BATCH_SIZE`)**: The process is now batch-oriented to maximize GPU utilization. A batch size of `16` is chosen as a safe default for an 8GB GPU, as it needs to hold the full-size feature maps in VRAM.\n",
    "- **Chunking**: `CHUNK_SIZE` now refers to the number of ROIs per `.pt` chunk file. This ensures each file is a manageable size for streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fbe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n",
      "Output will be written to: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "SEED = 2\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---- Paths ----\n",
    "ROOT = Path.cwd()\n",
    "DATA_YAML = ROOT / \"data_vehicle_hierarchy.yaml\"\n",
    "L3_CHECKPOINT = ROOT / \"runs\" / \"yolo_l3_base\" / \"weights\" / \"best.pt\"\n",
    "\n",
    "# Output dirs - NOW SAVING .pt FILES\n",
    "OUT_DIR = ROOT / \"aux_heads_chunks_pt\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREVIEW_DIR = OUT_DIR / \"preview\"\n",
    "PREVIEW_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Split-specific subfolders (train/val)\n",
    "TRAIN_OUT_DIR = OUT_DIR / \"train\"\n",
    "VAL_OUT_DIR = OUT_DIR / \"val\"\n",
    "TRAIN_OUT_DIR.mkdir(exist_ok=True)\n",
    "VAL_OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "STATS_TRAIN_PATH = OUT_DIR / \"stats_train.json\"\n",
    "STATS_VAL_PATH = OUT_DIR / \"stats_val.json\"\n",
    "CLASS_WEIGHTS_PATH = OUT_DIR / \"class_weights.json\"\n",
    "\n",
    "# Hierarchy\n",
    "L3_NAMES: List[str] = [\n",
    "    \"bus\",\"work_van\",\"single_unit_truck\",\"pickup_truck\",\n",
    "    \"articulated_truck\",\"car\",\"motorcycle\",\"bicycle\"\n",
    " ]\n",
    "L2_NAMES: List[str] = [\"heavy_vehicle\",\"car_group\",\"two_wheeled_vehicle\"]\n",
    "CLASS_TO_L2: Dict[str, str] = {\n",
    "    \"bus\": \"heavy_vehicle\", \"single_unit_truck\": \"heavy_vehicle\", \"articulated_truck\": \"heavy_vehicle\",\n",
    "    \"car\": \"car_group\", \"pickup_truck\": \"car_group\", \"work_van\": \"car_group\",\n",
    "    \"motorcycle\": \"two_wheeled_vehicle\", \"bicycle\": \"two_wheeled_vehicle\",\n",
    "}\n",
    "L1_NAMES: List[str] = [\"vehicle\"]\n",
    "\n",
    "# Index maps\n",
    "L3_TO_IDX = {n:i for i,n in enumerate(L3_NAMES)}\n",
    "L2_TO_IDX = {n:i for i,n in enumerate(L2_NAMES)}\n",
    "\n",
    "# Matching Thresholds\n",
    "# These IoU (Intersection over Union) thresholds define how we label ROI candidates.\n",
    "# A \"candidate\" can be a box predicted by the L3 model or a synthetically jittered box.\n",
    "# The gap between IOU_NEG and IOU_POS creates a \"dead zone\" for ambiguous overlaps, which are ignored.\n",
    "IOU_POS = 0.55      # Minimum IoU with a ground truth box to be considered a POSITIVE sample.\n",
    "                    # This teaches the aux heads tolerance for slight misalignments.\n",
    "IOU_NEG = 0.25      # Maximum IoU with any ground truth box to be considered a NEGATIVE sample.\n",
    "                    # This provides confident negative examples.\n",
    "IOU_BG  = 0.05      # Stricter maximum IoU for randomly sampled background boxes to ensure\n",
    "                    # they are \"pure\" background and do not contain parts of any object.\n",
    "\n",
    "# L3 Inference & Data Preparation Settings\n",
    "PREP_BATCH_SIZE = 16       # Number of images to process on the GPU in a single pass during this script.\n",
    "                           # A balance between speed (higher is faster) and VRAM usage.\n",
    "PRED_CONF_THRES = 0.01     # L3 model confidence threshold. Set very low to capture ALL potential detections,\n",
    "                           # including weak ones, which the aux heads will learn to either rescue or reject.\n",
    "PRED_IOU_NMS    = 0.70     # NMS IoU threshold for the L3 model. Set high to be permissive, allowing\n",
    "                           # multiple overlapping candidates for the aux heads to analyze.\n",
    "PRED_MAX_DET    = 300      # A safety cap on the maximum number of detections per image from the L3 model.\n",
    "PREP_WORKERS    = 8        # Number of CPU worker threads for loading data. Half of total threads is a good start.\n",
    "\n",
    "# Feature Extraction Configuration\n",
    "# Parameters for our \"Unified-Compact-ROI\" feature extraction pipeline.\n",
    "ROI_ALIGN_SIZE = (7, 7)    # The fixed spatial size (H, W) to which all ROIs are pooled by roi_align.\n",
    "                           # 7x7 is a standard size that preserves spatial patterns efficiently.\n",
    "COMPRESSED_DIM = 256       # The final dimension of the feature vector saved to disk. Our FeatureCompressor\n",
    "                           # CNN will process the 7x7 patch down to this compact size.\n",
    "PYRAMID_THRESHOLDS = (32.0, 96.0) # Pixel thresholds on a box's longest side to select the feature map:\n",
    "                                  #   - box_side <= 32: Use P3 (high-res map for small objects)\n",
    "                                  #   - 32 < box_side <= 96: Use P4 (medium-res map)\n",
    "                                  #   - box_side > 96: Use P5 (low-res map for large objects)\n",
    "\n",
    "# Jitter & Background Sampling\n",
    "# Parameters to control the generation of synthetic ROIs for data augmentation and hard-negative mining.\n",
    "JITTER_POS_PER_GT = 1      # Number of \"jittered positive\" boxes to generate per ground truth object.\n",
    "JITTER_NEG_PER_GT = 2      # Number of \"jittered negative\" boxes to generate per ground truth object.\n",
    "JITTER_SHIFT_RANGE = (0.1, 0.6) # A box's center will be randomly shifted by 10% to 60% of its size.\n",
    "JITTER_SCALE_RANGE = (0.1, 0.6) # A box's size will be randomly scaled up or down by 10% to 60%.\n",
    "BG_NEG_PER_IMG    = 3      # Number of random \"background negative\" boxes to sample per image.\n",
    "BG_MIN_SIZE_FRAC  = 0.05   # Minimum size of a background box, as a fraction of image width/height,\n",
    "                           # to prevent sampling tiny, meaningless patches.\n",
    "\n",
    "# Chunking (for .pt files)\n",
    "CHUNK_SIZE = 20_000   # ~20k ROI feature vectors per file\n",
    "MAX_IMAGES = None     # Set to small int to debug on subset\n",
    "\n",
    "print(\"Config loaded.\")\n",
    "print(\"Output will be written to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973af185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74746 train images.\n",
      "Found 13190 val images.\n"
     ]
    }
   ],
   "source": [
    "assert DATA_YAML.exists(), f\"Missing data yaml: {DATA_YAML}\"\n",
    "with open(DATA_YAML, \"r\") as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "\n",
    "# Expected YOLO structure in data yaml\n",
    "train_dir = Path(data_cfg[\"train\"])  # folder path or list file\n",
    "val_dir = Path(data_cfg.get(\"val\", \"\")) if \"val\" in data_cfg else None\n",
    "\n",
    "# Infer labels dir by standard YOLO layout\n",
    "# images/train/*.jpg -> labels/train/*.txt\n",
    "def infer_labels_dir(img_dir: Path) -> Path:\n",
    "    if \"images\" in img_dir.parts:\n",
    "        idx = img_dir.parts.index(\"images\")\n",
    "        labels = Path(*img_dir.parts[:idx], \"labels\", *img_dir.parts[idx+1:])\n",
    "        return labels\n",
    "    # fallback\n",
    "    return img_dir.parent / \"labels\" / img_dir.name\n",
    "\n",
    "labels_train = infer_labels_dir(train_dir)\n",
    "labels_val = infer_labels_dir(val_dir) if val_dir and str(val_dir) != \"\" else None\n",
    "\n",
    "assert train_dir.exists(), f\"Train images dir not found: {train_dir}\"\n",
    "assert labels_train.exists(), f\"Train labels dir not found: {labels_train}\"\n",
    "if val_dir and str(val_dir) != \"\":\n",
    "    assert val_dir.exists(), f\"Val images dir not found: {val_dir}\"\n",
    "    assert labels_val.exists(), f\"Val labels dir not found: {labels_val}\"\n",
    "\n",
    "# Collect image files\n",
    "IMG_EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\")\n",
    "train_images = [p for p in sorted(train_dir.glob(\"**/*\")) if p.suffix.lower() in IMG_EXTS]\n",
    "val_images = [p for p in sorted(val_dir.glob(\"**/*\")) if (val_dir and p.suffix.lower() in IMG_EXTS)] if (val_dir and str(val_dir) != \"\") else []\n",
    "if MAX_IMAGES is not None:\n",
    "    train_images = train_images[:MAX_IMAGES]\n",
    "    val_images = val_images[:MAX_IMAGES]\n",
    "\n",
    "print(f\"Found {len(train_images)} train images.\")\n",
    "print(f\"Found {len(val_images)} val images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e60f2",
   "metadata": {},
   "source": [
    "## Negative Sampling Strategies\n",
    "\n",
    "To ensure the **auxiliary L2/L1 heads** become **resilient to false positives from L3**, the dataset builder implements several complementary **negative sampling strategies**. Each is carefully designed to mimic the kinds of mistakes the base detector might make at inference:\n",
    "\n",
    "1. **False Positive Predictions (`pred_fp`)**  \n",
    "   - Collected directly from L3’s detections that **do not overlap any ground truth** sufficiently (IoU ≤ 0.3).  \n",
    "   - These represent *realistic mistakes* the L3 head produces, teaching the aux heads to recognize and down-weight such spurious detections.\n",
    "\n",
    "2. **Jittered Negatives (`jitter_neg`)**  \n",
    "   - Generated by perturbing ground-truth boxes (shift/scale up to 20%).  \n",
    "   - If the perturbed box has **low overlap with its GT (IoU ≤ 0.3)**, it is labeled as a *negative*.  \n",
    "   - Simulates the situation where L3 places a box in the right neighborhood but **off-target enough to be wrong**.\n",
    "\n",
    "3. **Ignored Ambiguities (0.3 < IoU < 0.5)**  \n",
    "   - Predictions or jitters with intermediate IoU are **not included** in training.  \n",
    "   - This avoids noisy labels in the “gray zone” and keeps supervision **clean and decisive**.\n",
    "\n",
    "4. **Background Negatives (`bg_neg`)**  \n",
    "   - Random boxes are sampled in the image, sized roughly like typical objects, but constrained to have **IoU ≤ 0.05 with any GT**.  \n",
    "   - These are **pure background crops** (road, sky, walls, etc.), helping the aux heads avoid firing on areas with **no objects at all**.\n",
    "\n",
    "\n",
    "\n",
    "### Why This Matters\n",
    "- **Coverage**: The heads see *real mistakes* (FPs), *plausible misalignments* (jitter-neg), and *true background* (bg-neg).  \n",
    "- **Robustness**: Aux heads learn to tolerate small misalignment (positives at IoU ≥ 0.5), but to veto boxes that are clearly wrong.  \n",
    "- **Balance**: Ignoring the ambiguous range prevents confusing signals and stabilizes learning.\n",
    "\n",
    "This combination ensures that aux heads act as **robust validators**:  \n",
    "- They pass through true objects (even with slight misalignment),  \n",
    "- But suppress random clutter and false alarms.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c2e4a",
   "metadata": {},
   "source": [
    "## Positive Sampling Strategies\n",
    "\n",
    "In addition to negatives, the dataset builder ensures that **auxiliary L2/L1 heads** receive strong and diverse **positive examples**, so they learn to *accept true objects* even when L3’s boxes are slightly imperfect:\n",
    "\n",
    "1. **True Positives (`gt_pos`)**  \n",
    "   - Directly taken from the **ground-truth (GT) annotations**.  \n",
    "   - Serve as the **gold standard positives**, ensuring the aux heads learn to recognize each object correctly within the hierarchy.\n",
    "\n",
    "2. **Jittered Positives (`jitter_pos`)**  \n",
    "   - Perturbed versions of GT boxes (shifted/scaled by up to 20%).  \n",
    "   - If the perturbed box maintains **sufficient overlap (IoU ≥ 0.5)** with the GT, it is treated as a **positive sample**.  \n",
    "   - This teaches aux heads to be **tolerant to mild misalignments**, preparing them to accept real detections from L3 that are not pixel-perfect but still correct.\n",
    "\n",
    "3. **IoU-Aware Filtering**  \n",
    "   - Boxes in the ambiguous overlap range (0.3 < IoU < 0.5) are **ignored** rather than forced into positive/negative.  \n",
    "   - Keeps training labels **clean**, avoiding confusing supervision that could make aux heads overly strict.\n",
    "\n",
    "### Why This Matters\n",
    "- **Tolerance**: By including jittered positives, aux heads won’t wrongly reject slightly off detections that are still valid.  \n",
    "- **Generalization**: They learn not just from exact GT, but also from realistic detector outputs.  \n",
    "- **Consistency**: The IoU-aware scheme ensures stable training by distinguishing true objects from noise without ambiguity.\n",
    "\n",
    "This guarantees that aux heads act as **forgiving validators**:  \n",
    "- They correctly accept slightly misaligned but true boxes,  \n",
    "- While still rejecting boxes that drift too far into the false positive zone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d595b7e",
   "metadata": {},
   "source": [
    "## Helper Functions (IOU, Box Ops, Pyramid Chooser)\n",
    "\n",
    "This cell contains core utility functions. We have added the `choose_pyramid_level` helper, which is a key component of our scale-aware feature extraction strategy.\n",
    "\n",
    "### Engineering Decision:\n",
    "- **Scale-Aware Pooling**: Instead of wastefully pooling from all three feature maps, this function uses simple box size heuristics to select the *single most appropriate* feature map (P3 for small objects, P4 for medium, P5 for large). This is a standard FPN technique that significantly improves efficiency while ensuring features are extracted at the correct resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import roi_align\n",
    "\n",
    "def yolo_txt_to_boxes(label_path: Path, img_w: int, img_h: int):\n",
    "    # (Existing function, no changes needed)\n",
    "    if not label_path.exists(): return []\n",
    "    lines = label_path.read_text().strip().splitlines()\n",
    "    out = []\n",
    "    for ln in lines:\n",
    "        parts = ln.strip().split()\n",
    "        if len(parts) < 5: continue\n",
    "        c = int(float(parts[0]))\n",
    "        xc, yc, w, h = [float(p) for p in parts[1:5]]\n",
    "        x1 = max(0.0, (xc - w/2.0) * img_w)\n",
    "        y1 = max(0.0, (yc - h/2.0) * img_h)\n",
    "        x2 = min(float(img_w), (xc + w/2.0) * img_w)\n",
    "        y2 = min(float(img_h), (yc + h/2.0) * img_h)\n",
    "        if x2 > x1 and y2 > y1:\n",
    "            out.append((c, [x1,y1,x2,y2]))\n",
    "    return out\n",
    "\n",
    "def boxes_iou_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    # (Existing function, no changes needed)\n",
    "    if len(a)==0 or len(b)==0: return np.zeros((len(a), len(b)), dtype=np.float32)\n",
    "    x1 = np.maximum(a[:,None,0], b[None,:,0])\n",
    "    y1 = np.maximum(a[:,None,1], b[None,:,1])\n",
    "    x2 = np.minimum(a[:,None,2], b[None,:,2])\n",
    "    y2 = np.minimum(a[:,None,3], b[None,:,3])\n",
    "    inter = np.clip(x2 - x1, 0, None) * np.clip(y2 - y1, 0, None)\n",
    "    area_a = (a[:,2]-a[:,0]) * (a[:,3]-a[:,1])\n",
    "    area_b = (b[:,2]-b[:,0]) * (b[:,3]-b[:,1])\n",
    "    union = area_a[:,None] + area_b[None,:] - inter\n",
    "    iou = np.where(union > 0, inter/union, 0.0)\n",
    "    return iou.astype(np.float32)\n",
    "\n",
    "def clip_box_xyxy(xyxy, w, h):\n",
    "    # (Existing function, no changes needed)\n",
    "    x1,y1,x2,y2 = xyxy\n",
    "    x1 = float(np.clip(x1, 0, w-1)); y1 = float(np.clip(y1, 0, h-1))\n",
    "    x2 = float(np.clip(x2, 0, w-1)); y2 = float(np.clip(y2, 0, h-1))\n",
    "    if x2 <= x1 or y2 <= y1: return None\n",
    "    return [x1,y1,x2,y2]\n",
    "\n",
    "def to_one_hot(idx: Optional[int], n: int):\n",
    "    # (Existing function, no changes needed)\n",
    "    v = [0]*n\n",
    "    if idx is not None and 0 <= idx < n: v[idx] = 1\n",
    "    return v\n",
    "\n",
    "# NEW HELPER FUNCTION\n",
    "def choose_pyramid_level(boxes_xyxy: torch.Tensor, thresholds: Tuple[float, float] = (32.0, 96.0)):\n",
    "    \"\"\"\n",
    "    Choose which pyramid level to use based on box size.\n",
    "    Args:\n",
    "        boxes_xyxy: Float tensor [N, 4] (x1, y1, x2, y2) in pixels.\n",
    "        thresholds: tuple(t1, t2) thresholds (in pixels).\n",
    "                    if max(w,h) <= t1: use P3 (high-res)\n",
    "                    elif <= t2: use P4\n",
    "                    else: P5\n",
    "    Returns:\n",
    "        levels: Long tensor [N] with values in {3, 4, 5}\n",
    "    \"\"\"\n",
    "    if boxes_xyxy.numel() == 0:\n",
    "        return torch.empty(0, dtype=torch.long, device=boxes_xyxy.device)\n",
    "    \n",
    "    w = boxes_xyxy[:, 2] - boxes_xyxy[:, 0]\n",
    "    h = boxes_xyxy[:, 3] - boxes_xyxy[:, 1]\n",
    "    max_side = torch.maximum(w, h)\n",
    "\n",
    "    t1, t2 = thresholds\n",
    "    levels = torch.full_like(max_side, 4, dtype=torch.long) # Default to P4\n",
    "    levels[max_side <= t1] = 3\n",
    "    levels[max_side > t2] = 5\n",
    "    return levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8a450",
   "metadata": {},
   "source": [
    "## Jitter & Background Samplers (IoU-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4893761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter_box(xyxy, w, h, shift_frac=0.2, scale_frac=0.2):\n",
    "    \"\"\"\n",
    "    Randomly jitter a box by shifting center and scaling size.\n",
    "    shift_frac: max absolute fraction for center shift relative to box w/h.\n",
    "    scale_frac: max absolute fraction for scaling (±).\n",
    "    \"\"\"\n",
    "    x1,y1,x2,y2 = xyxy\n",
    "    bw = x2 - x1\n",
    "    bh = y2 - y1\n",
    "    cx = x1 + bw/2.0\n",
    "    cy = y1 + bh/2.0\n",
    "\n",
    "    # sample actual magnitudes (already provided by caller)\n",
    "    # shift\n",
    "    dx = (random.uniform(-shift_frac, shift_frac)) * bw\n",
    "    dy = (random.uniform(-shift_frac, shift_frac)) * bh\n",
    "    # scale\n",
    "    sx = 1.0 + (random.uniform(-scale_frac, scale_frac))\n",
    "    sy = 1.0 + (random.uniform(-scale_frac, scale_frac))\n",
    "\n",
    "    new_bw = max(2.0, bw * sx)\n",
    "    new_bh = max(2.0, bh * sy)\n",
    "    ncx = cx + dx\n",
    "    ncy = cy + dy\n",
    "\n",
    "    nx1 = ncx - new_bw/2.0\n",
    "    ny1 = ncy - new_bh/2.0\n",
    "    nx2 = ncx + new_bw/2.0\n",
    "    ny2 = ncy + new_bh/2.0\n",
    "\n",
    "    clipped = clip_box_xyxy([nx1,ny1,nx2,ny2], w, h)\n",
    "    return clipped\n",
    "\n",
    "def sample_bg_boxes(w, h, n=3, min_size_frac=0.05, size_ref=None):\n",
    "    \"\"\"\n",
    "    Sample random background boxes. If size_ref is provided (list of gt sizes),\n",
    "    sample sizes near typical GT sizes; else use a broad range.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for _ in range(n*3):  # try more to meet IoU constraints later\n",
    "        if size_ref and len(size_ref)>0:\n",
    "            sw, sh = random.choice(size_ref)\n",
    "            # vary by ±30%\n",
    "            sw = max(4.0, sw * random.uniform(0.7, 1.3))\n",
    "            sh = max(4.0, sh * random.uniform(0.7, 1.3))\n",
    "        else:\n",
    "            min_w = max(4.0, w * min_size_frac)\n",
    "            min_h = max(4.0, h * min_size_frac)\n",
    "            # pick random size\n",
    "            sw = random.uniform(min_w, w * 0.5)\n",
    "            sh = random.uniform(min_h, h * 0.5)\n",
    "        cx = random.uniform(sw/2.0, w - sw/2.0)\n",
    "        cy = random.uniform(sh/2.0, h - sh/2.0)\n",
    "        x1 = cx - sw/2.0\n",
    "        y1 = cy - sh/2.0\n",
    "        x2 = cx + sw/2.0\n",
    "        y2 = cy + sh/2.0\n",
    "        b = clip_box_xyxy([x1,y1,x2,y2], w, h)\n",
    "        if b is not None:\n",
    "            out.append(b)\n",
    "        if len(out) >= n:\n",
    "            break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fea9c",
   "metadata": {},
   "source": [
    "##  Feature Compressor Definition\n",
    "\n",
    "This cell defines the `FeatureCompressor` module. This lightweight CNN is a critical part of our pipeline.\n",
    "\n",
    "### Engineering Decision:\n",
    "- **Pre-computation**: Instead of saving large `7x7` feature patches to disk and processing them during training, we use this module *once* during data preparation. It takes the `roi_align` output and distills it into a compact, fixed-size vector (`256-dim`).\n",
    "- **Efficiency**: This strategy solves the I/O and storage bottleneck. We save only the small, final vectors, allowing the training script to be extremely fast and memory-efficient, enabling large batch sizes. The compressor model itself is moved to the GPU for fast processing during this data prep stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53521f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureCompressor class defined. Will be instantiated after model loading.\n"
     ]
    }
   ],
   "source": [
    "class FeatureCompressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Compresses a [N, C, H, W] feature map from ROI Align into a compact [N, D] vector.\n",
    "    This is used once during data preparation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_dim: int, hidden_dim_factor: int = 2):\n",
    "        super().__init__()\n",
    "        hidden_dim = out_dim * hidden_dim_factor\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, out_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [N, C, H, W] from roi_align\n",
    "        features = self.trunk(x)\n",
    "        pooled_features = self.pool(features)\n",
    "        return torch.flatten(pooled_features, 1) # [N, out_dim]\n",
    "\n",
    "# We will define the in_channels later once we hook the model\n",
    "# For now, we create a placeholder for the variable.\n",
    "feature_compressor = None\n",
    "print(\"FeatureCompressor class defined. Will be instantiated after model loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d370f9",
   "metadata": {},
   "source": [
    "## Model Loading, Feature Hooking & Compressor Instantiation\n",
    "\n",
    "This cell now performs three critical setup tasks:\n",
    "1.  **Loads the pre-trained L3 YOLO Model** as before.\n",
    "2.  **Identifies and Hooks Neck Layers**: We programmatically find the specific layers in the YOLOv11 neck that produce the P3, P4, and P5 feature maps. We then attach **forward hooks** to these layers. A hook is a PyTorch function that runs during the model's forward pass, allowing us to capture intermediate outputs (the feature maps) without modifying the original model's code. This is a clean and robust way to get the data we need.\n",
    "3.  **Instantiates Feature Compressors**: Once the hooks are in place and we know the exact channel dimensions of P3, P4, and P5, we can properly instantiate our `FeatureCompressor` models. We create a dictionary of three separate compressors, one for each pyramid level, and move them to the GPU, ready for the data generation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d147f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded L3 model: best.pt\n",
      "Successfully attached forward hooks to layers 22(P3), 19(P4), 16(P5).\n",
      "\n",
      "Running a single-image probe to capture feature map shapes...\n",
      "Captured feature map shapes (B, C, H, W) and strides:\n",
      "  P3: torch.Size([1, 256, 14, 20]), Stride: 8.0\n",
      "  P4: torch.Size([1, 128, 28, 40]), Stride: 16.0\n",
      "  P5: torch.Size([1, 64, 56, 80]), Stride: 32.0\n",
      "\n",
      "FeatureCompressor models instantiated and moved to cuda:0.\n",
      "Total parameters in all compressors: 5,607,936\n",
      "\n",
      "Phase 2 setup complete. Ready for data generation.\n"
     ]
    }
   ],
   "source": [
    "# This dictionary will store the feature maps captured by our hooks\n",
    "feature_maps = {}\n",
    "# This dictionary will store the instantiated FeatureCompressor modules\n",
    "feature_compressors = {}\n",
    "# This dictionary will store the spatial stride for each feature map\n",
    "strides = {}\n",
    "\n",
    "# 1. Load the L3 Model\n",
    "assert L3_CHECKPOINT.exists(), f\"Missing L3 checkpoint: {L3_CHECKPOINT}\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLO(str(L3_CHECKPOINT))\n",
    "model.to(DEVICE)\n",
    "print(\"Loaded L3 model:\", L3_CHECKPOINT.name)\n",
    "\n",
    "# 2. Identify and Hook Neck Layers\n",
    "# The hook function is a closure that captures the output of a specific layer\n",
    "def get_features_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        # For some YOLO versions, the neck returns a tuple. We take the tensor output.\n",
    "        feature_maps[name] = output[0] if isinstance(output, tuple) else output\n",
    "    return hook\n",
    "\n",
    "# =================================================================================\n",
    "# TODO: UPDATE THESE INDICES based on the architcture of used model!\n",
    "# =================================================================================\n",
    "p5_module_idx = 16\n",
    "p4_module_idx = 19\n",
    "p3_module_idx = 22\n",
    "# =================================================================================\n",
    "\n",
    "try:\n",
    "    hook_p5 = model.model.model[p5_module_idx].register_forward_hook(get_features_hook('p5'))\n",
    "    hook_p4 = model.model.model[p4_module_idx].register_forward_hook(get_features_hook('p4'))\n",
    "    hook_p3 = model.model.model[p3_module_idx].register_forward_hook(get_features_hook('p3'))\n",
    "    print(f\"Successfully attached forward hooks to layers {p3_module_idx}(P3), {p4_module_idx}(P4), {p5_module_idx}(P5).\")\n",
    "\n",
    "except IndexError as e:\n",
    "    print(f\"\\n[ERROR] IndexError: {e}\")\n",
    "    print(\"The indices you provided are incorrect for this model's architecture.\")\n",
    "    print(\"Please re-run the 'Probe' cell, carefully identify the correct indices, and update them.\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"Could not attach hooks. Error: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Quick Probe to Get Channel Dims and Instantiate Compressors\n",
    "if len(train_images) > 0:\n",
    "    print(\"\\nRunning a single-image probe to capture feature map shapes...\")\n",
    "    # Using model.predict() is the easiest way to trigger a full forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model.predict(source=[str(train_images[0])], device=DEVICE, verbose=False)\n",
    "\n",
    "    if not feature_maps:\n",
    "        raise RuntimeError(\"Hooks did not capture any feature maps. Check module indices.\")\n",
    "\n",
    "    # Get channel dimensions from the captured feature maps\n",
    "    p3_channels = feature_maps['p3'].shape[1]\n",
    "    p4_channels = feature_maps['p4'].shape[1]\n",
    "    p5_channels = feature_maps['p5'].shape[1]\n",
    "    \n",
    "    strides = {\n",
    "        'p3': model.model.stride[0], # Stride 8\n",
    "        'p4': model.model.stride[1], # Stride 16\n",
    "        'p5': model.model.stride[2], # Stride 32\n",
    "    }\n",
    "\n",
    "    print(f\"Captured feature map shapes (B, C, H, W) and strides:\")\n",
    "    print(f\"  P3: {feature_maps['p3'].shape}, Stride: {strides['p3']}\")\n",
    "    print(f\"  P4: {feature_maps['p4'].shape}, Stride: {strides['p4']}\")\n",
    "    print(f\"  P5: {feature_maps['p5'].shape}, Stride: {strides['p5']}\")\n",
    "\n",
    "    # Now, instantiate the FeatureCompressor for each pyramid level\n",
    "    feature_compressors = nn.ModuleDict({\n",
    "        'p3': FeatureCompressor(in_channels=p3_channels, out_dim=COMPRESSED_DIM),\n",
    "        'p4': FeatureCompressor(in_channels=p4_channels, out_dim=COMPRESSED_DIM),\n",
    "        'p5': FeatureCompressor(in_channels=p5_channels, out_dim=COMPRESSED_DIM)\n",
    "    }).to(DEVICE)\n",
    "    feature_compressors.eval() # Set to eval mode as we only use it for inference here\n",
    "    \n",
    "    total_params = sum(p.numel() for p in feature_compressors.parameters())\n",
    "    print(f\"\\nFeatureCompressor models instantiated and moved to {DEVICE}.\")\n",
    "    print(f\"Total parameters in all compressors: {total_params:,}\")\n",
    "\n",
    "else:\n",
    "    print(\"No training images found, skipping probe and compressor instantiation.\")\n",
    "\n",
    "# 4. Final Sanity Check\n",
    "print(\"\\nPhase 2 setup complete. Ready for data generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06422627",
   "metadata": {},
   "source": [
    "## Ground Truth & Labeling Helpers\n",
    "\n",
    "This cell contains the helper functions responsible for loading ground truth annotations from YOLO `.txt` files and mapping the L3 leaf classes to their L2 parent classes in the hierarchy. This logic is essential for creating the correct training targets for our auxiliary heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f0c4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l3_to_l2_idx(l3_idx: int) -> Optional[int]:\n",
    "    \"\"\"Maps an L3 class index to its corresponding L2 class index.\"\"\"\n",
    "    if l3_idx is None or l3_idx < 0 or l3_idx >= len(L3_NAMES):\n",
    "        return None\n",
    "    l3_name = L3_NAMES[l3_idx]\n",
    "    l2_name = CLASS_TO_L2.get(l3_name, None)\n",
    "    return L2_TO_IDX[l2_name] if l2_name in L2_TO_IDX else None\n",
    "\n",
    "def gt_to_l2_idx_by_name(l3_name: str) -> Optional[int]:\n",
    "    \"\"\"Maps an L3 class name to its corresponding L2 class index.\"\"\"\n",
    "    l2_name = CLASS_TO_L2.get(l3_name, None)\n",
    "    return L2_TO_IDX[l2_name] if l2_name in L2_TO_IDX else None\n",
    "\n",
    "def prepare_gt(image_path: Path, labels_dir: Path):\n",
    "    \"\"\"\n",
    "    Loads an image to get its dimensions and parses the corresponding\n",
    "    YOLO label file to get ground truth boxes and their hierarchical labels.\n",
    "    \"\"\"\n",
    "    # Get image size\n",
    "    try:\n",
    "        with Image.open(image_path) as im:\n",
    "            w, h = im.size\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not open image {image_path}. Skipping. Error: {e}\")\n",
    "        return [], (0,0), []\n",
    "\n",
    "    # Find the corresponding label path\n",
    "    # Standard: images/train/xxx.jpg -> labels/train/xxx.txt\n",
    "    lbl_path = labels_dir / image_path.with_suffix(\".txt\").name\n",
    "    # Handle nested directories if they exist\n",
    "    if not lbl_path.exists():\n",
    "        try:\n",
    "            # Assumes structure like .../images/split_name/subdir/img.jpg\n",
    "            # Maps to .../labels/split_name/subdir/img.txt\n",
    "            split_name = image_path.parent.parent.name # e.g., 'train' or 'val'\n",
    "            relative_path = image_path.relative_to(image_path.parent.parent.parent / \"images\" / split_name)\n",
    "            lbl_path = labels_dir.parent / split_name / relative_path.with_suffix(\".txt\")\n",
    "        except Exception:\n",
    "            # Fallback if path logic is complex\n",
    "            pass\n",
    "\n",
    "    # Parse GT boxes from the label file\n",
    "    gt_lst = yolo_txt_to_boxes(lbl_path, w, h)\n",
    "\n",
    "    # Final list of GTs with (l3_idx, l2_idx, box_xyxy)\n",
    "    gt = []\n",
    "    # List of box sizes (w,h) for sampling background boxes\n",
    "    size_ref = []\n",
    "    for cls_id, b in gt_lst:\n",
    "        if cls_id < 0 or cls_id >= len(L3_NAMES):\n",
    "            continue\n",
    "        l3_name = L3_NAMES[cls_id]\n",
    "        l2_idx = gt_to_l2_idx_by_name(l3_name)\n",
    "        if l2_idx is not None:\n",
    "            gt.append( (cls_id, l2_idx, b) )\n",
    "            bw = max(1.0, b[2]-b[0])\n",
    "            bh = max(1.0, b[3]-b[1])\n",
    "            size_ref.append((bw, bh))\n",
    "            \n",
    "    return gt, (w,h), size_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96136afd",
   "metadata": {},
   "source": [
    "## Core Builder: Batched Feature Extraction (Final Version)\n",
    "\n",
    "This is the final, definitive version of the core data preparation logic. It uses a robust manual batching loop and includes all necessary metadata for downstream tasks.\n",
    "\n",
    "### Key Engineering Decisions:\n",
    "- **Manual Batching**: We iterate through image paths in batches, calling `model.predict()` on one batch at a time. This is efficient and avoids the \"Too many open files\" error.\n",
    "- **Pre-filtering for `roi_align`**: We identify which ROIs belong to a specific pyramid level **before** running `roi_align`, which is both correct and computationally efficient.\n",
    "- **Complete Metadata**: Crucially, we now save the source `img_path` along with the type, IoU, and label for **every single ROI**. This is essential for visualization, debugging, and advanced analysis later.\n",
    "- **Detailed Progress Monitoring**: The `tqdm` progress bar tracks the running total of each ROI type, giving a clear view of the dataset composition as it's being built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing split: train: 100%|██████████| 74746/74746 [17:36<00:00, 70.75it/s, gt_pos=188.5k, pred_tp=181.3k, pred_fp=13.8k, jitter_neg=41.6k, jitter_pos=77.0k, bg_neg=175.8k, Total_ROIs=0.68M]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished split train. Total ROIs: 678015. Chunks: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing split: val: 100%|██████████| 13190/13190 [02:42<00:00, 80.94it/s, gt_pos=48.2k, pred_tp=44.1k, jitter_pos=19.6k, jitter_neg=10.6k, bg_neg=31.5k, pred_fp=3.3k, Total_ROIs=0.16M]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished split val. Total ROIs: 157400. Chunks: 8\n",
      "\n",
      "Finished all processing. Train chunks: 34, Val chunks: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# This is the final, corrected version of the core builder.\n",
    "# It includes detailed reporting and saves the 'img_path' in the metadata.\n",
    "\n",
    "def build_chunks_for_split(image_paths: List[Path], labels_dir: Path, out_dir: Path) -> List[str]:\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    buffer = {\n",
    "        'features': [], 'l1_targets': [], 'l2_targets': [],\n",
    "        'boxes': [], 'metadata': []\n",
    "    }\n",
    "    chunk_idx = 0\n",
    "    chunk_paths_local: List[str] = []\n",
    "    total_rois_processed = 0\n",
    "    stats_counter = Counter()\n",
    "\n",
    "    pbar = tqdm(total=len(image_paths), desc=f\"Processing split: {out_dir.name}\")\n",
    "\n",
    "    # Manually iterate through image paths in batches\n",
    "    for i in range(0, len(image_paths), PREP_BATCH_SIZE):\n",
    "        batch_paths = image_paths[i : i + PREP_BATCH_SIZE]\n",
    "        if not batch_paths: continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results_list = model.predict(source=[str(p) for p in batch_paths], device=DEVICE, verbose=False)\n",
    "\n",
    "        batch_rois_by_image = [[] for _ in batch_paths]\n",
    "        batch_metadata_by_image = [[] for _ in batch_paths]\n",
    "\n",
    "        for batch_idx, res in enumerate(results_list):\n",
    "            img_path = Path(res.path)\n",
    "            gt, (w, h), size_ref = prepare_gt(img_path, labels_dir)\n",
    "            gt_boxes = np.array([g[2] for g in gt]) if gt else np.zeros((0, 4))\n",
    "            gt_l3s, gt_l2s = [g[0] for g in gt], [g[1] for g in gt]\n",
    "            rois_this_image = []\n",
    "            \n",
    "            # 1. GT boxes\n",
    "            for gt_l3, gt_l2, gxy in gt:\n",
    "                rois_this_image.append((gxy, \"gt_pos\", 1.0, gt_l3, gt_l2)); stats_counter[\"gt_pos\"] += 1\n",
    "            # 2. Predicted boxes\n",
    "            boxes = res.boxes\n",
    "            pred_xyxy = boxes.xyxy.cpu().numpy() if boxes is not None else np.zeros((0, 4))\n",
    "            if pred_xyxy.size > 0 and gt_boxes.size > 0:\n",
    "                iou_mat = boxes_iou_matrix(pred_xyxy, gt_boxes); best_iou, best_gt = iou_mat.max(axis=1), iou_mat.argmax(axis=1)\n",
    "            else:\n",
    "                best_iou, best_gt = np.zeros(len(pred_xyxy)), -np.ones(len(pred_xyxy), dtype=int)\n",
    "            matched_gts = set()\n",
    "            for k in range(len(pred_xyxy)):\n",
    "                iou, j = float(best_iou[k]), int(best_gt[k])\n",
    "                if j != -1 and iou >= IOU_POS and j not in matched_gts:\n",
    "                    matched_gts.add(j); rois_this_image.append((pred_xyxy[k].tolist(), \"pred_tp\", iou, gt_l3s[j], gt_l2s[j])); stats_counter[\"pred_tp\"] += 1\n",
    "                elif iou <= IOU_NEG:\n",
    "                    rois_this_image.append((pred_xyxy[k].tolist(), \"pred_fp\", iou, None, None)); stats_counter[\"pred_fp\"] += 1\n",
    "            # 3. Jittered & Background boxes\n",
    "            for gt_l3, gt_l2, gxy in gt:\n",
    "                for _ in range(JITTER_POS_PER_GT):\n",
    "                    if jb := jitter_box(gxy, w, h, random.uniform(*JITTER_SHIFT_RANGE), random.uniform(*JITTER_SCALE_RANGE)):\n",
    "                        iou = float(boxes_iou_matrix(np.array([jb]), np.array([gxy]))[0, 0])\n",
    "                        if iou >= IOU_POS: rois_this_image.append((jb, \"jitter_pos\", iou, gt_l3, gt_l2)); stats_counter[\"jitter_pos\"] += 1\n",
    "                        elif iou <= IOU_NEG: rois_this_image.append((jb, \"jitter_neg\", iou, None, None)); stats_counter[\"jitter_neg\"] += 1\n",
    "                for _ in range(JITTER_NEG_PER_GT):\n",
    "                     if jb := jitter_box(gxy, w, h, random.uniform(*JITTER_SHIFT_RANGE), random.uniform(*JITTER_SCALE_RANGE)):\n",
    "                        iou = float(boxes_iou_matrix(np.array([jb]), np.array([gxy]))[0, 0])\n",
    "                        if iou <= IOU_NEG: rois_this_image.append((jb, \"jitter_neg\", iou, None, None)); stats_counter[\"jitter_neg\"] += 1\n",
    "            bg_boxes = sample_bg_boxes(w, h, n=BG_NEG_PER_IMG, size_ref=size_ref)\n",
    "            if bg_boxes:\n",
    "                iou_max = boxes_iou_matrix(np.array(bg_boxes), gt_boxes).max(axis=1) if gt_boxes.size > 0 else np.zeros(len(bg_boxes))\n",
    "                for box, iou in zip(bg_boxes, iou_max):\n",
    "                    if iou <= IOU_BG: rois_this_image.append((box, \"bg_neg\", float(iou), None, None)); stats_counter[\"bg_neg\"] += 1\n",
    "            if rois_this_image:\n",
    "                batch_rois_by_image[batch_idx] = [r[0] for r in rois_this_image]\n",
    "                batch_metadata_by_image[batch_idx] = [r[1:] for r in rois_this_image]\n",
    "\n",
    "        all_rois_flat = torch.cat([torch.tensor(rois, dtype=torch.float32, device=DEVICE) for rois in batch_rois_by_image], dim=0)\n",
    "        if all_rois_flat.numel() == 0:\n",
    "            pbar.update(len(batch_paths)); continue\n",
    "            \n",
    "        levels = choose_pyramid_level(all_rois_flat, thresholds=PYRAMID_THRESHOLDS)\n",
    "        final_features = torch.zeros(all_rois_flat.shape[0], COMPRESSED_DIM, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            for level_idx in [3, 4, 5]:\n",
    "                level_name = f'p{level_idx}'; mask = (levels == level_idx)\n",
    "                if not mask.any(): continue\n",
    "                original_indices = mask.nonzero().squeeze(1)\n",
    "                roi_counts = [len(r) for r in batch_rois_by_image]\n",
    "                image_indices_for_rois = torch.repeat_interleave(torch.arange(len(roi_counts), device=DEVICE), torch.tensor(roi_counts, device=DEVICE))\n",
    "                boxes_on_level = all_rois_flat[mask]\n",
    "                image_indices_on_level = image_indices_for_rois[mask]\n",
    "                rois_for_align_this_level = [boxes_on_level[image_indices_on_level == i] for i in range(len(batch_paths))]\n",
    "                pooled_feats = roi_align(feature_maps[level_name], rois_for_align_this_level, output_size=ROI_ALIGN_SIZE, spatial_scale=1.0 / strides[level_name])\n",
    "                if pooled_feats.numel() > 0:\n",
    "                    compressed_feats = feature_compressors[level_name](pooled_feats)\n",
    "                    final_features[original_indices] = compressed_feats\n",
    "\n",
    "        buffer['features'].append(final_features.cpu())\n",
    "        buffer['boxes'].append(all_rois_flat.cpu())\n",
    "        \n",
    "        # Populate Buffer with full metadata\n",
    "        current_rois_count = 0\n",
    "        for batch_idx, metadata_list in enumerate(batch_metadata_by_image):\n",
    "            if not metadata_list: continue\n",
    "            \n",
    "            img_path = Path(results_list[batch_idx].path) # Get the correct img_path for this set of ROIs\n",
    "            \n",
    "            for sample_type, iou, gt_l3, gt_l2 in metadata_list:\n",
    "                is_positive = gt_l2 is not None and iou >= IOU_POS\n",
    "                l1 = 1 if is_positive else 0\n",
    "                l2 = to_one_hot(gt_l2 if is_positive else None, len(L2_NAMES))\n",
    "                buffer['l1_targets'].append(l1)\n",
    "                buffer['l2_targets'].append(l2)\n",
    "                # Add img_path to each metadata entry\n",
    "                buffer['metadata'].append({\n",
    "                    'img_path': str(img_path),\n",
    "                    'type': sample_type,\n",
    "                    'iou': iou,\n",
    "                    'gt_cls_l2': gt_l2 if is_positive else None\n",
    "                })\n",
    "        \n",
    "        total_rois_processed += all_rois_flat.shape[0]; pbar.update(len(batch_paths))\n",
    "        postfix_stats = {k: f\"{v/1e3:.1f}k\" for k, v in stats_counter.items()}\n",
    "        postfix_stats['Total_ROIs'] = f\"{total_rois_processed/1e6:.2f}M\"\n",
    "        pbar.set_postfix(postfix_stats)\n",
    "\n",
    "        if len(buffer['metadata']) >= CHUNK_SIZE:\n",
    "            save_path = out_dir / f\"rois_chunk_{chunk_idx:04d}.pt\"\n",
    "            chunk_data = {'features': torch.cat(buffer['features'], dim=0).half(), 'l1_targets': torch.tensor(buffer['l1_targets'], dtype=torch.bool), 'l2_targets': torch.tensor(buffer['l2_targets'], dtype=torch.bool), 'boxes': torch.cat(buffer['boxes'], dim=0), 'metadata': buffer['metadata']}\n",
    "            torch.save(chunk_data, save_path); chunk_paths_local.append(str(save_path))\n",
    "            buffer = {k: [] for k in buffer}; chunk_idx += 1\n",
    "\n",
    "    if buffer['features']:\n",
    "        save_path = out_dir / f\"rois_chunk_{chunk_idx:04d}.pt\"\n",
    "        chunk_data = {'features': torch.cat(buffer['features'], dim=0).half(), 'l1_targets': torch.tensor(buffer['l1_targets'], dtype=torch.bool), 'l2_targets': torch.tensor(buffer['l2_targets'], dtype=torch.bool), 'boxes': torch.cat(buffer['boxes'], dim=0), 'metadata': buffer['metadata']}\n",
    "        torch.save(chunk_data, save_path); chunk_paths_local.append(str(save_path))\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"Finished split {out_dir.name}. Total ROIs: {total_rois_processed}. Chunks: {len(chunk_paths_local)}\")\n",
    "    return chunk_paths_local\n",
    "\n",
    "# Main Execution Logic\n",
    "assert len(train_images) > 0, \"No train images found.\"\n",
    "train_chunk_paths = build_chunks_for_split(train_images, labels_train, TRAIN_OUT_DIR)\n",
    "val_chunk_paths = []\n",
    "if len(val_images) > 0 and labels_val is not None:\n",
    "    val_chunk_paths = build_chunks_for_split(val_images, labels_val, VAL_OUT_DIR)\n",
    "print(f\"\\nFinished all processing. Train chunks: {len(train_chunk_paths)}, Val chunks: {len(val_chunk_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950190f2",
   "metadata": {},
   "source": [
    "## Quick Visual Sanity Check (from `.pt` Chunks)\n",
    "\n",
    "This cell provides a visual confirmation that our data generation is correct. It now loads a random `.pt` chunk, groups the ROIs by their source image, and draws the bounding boxes with colors corresponding to their sample type (`gt_pos`, `pred_fp`, etc.). This allows us to quickly verify that our sampling strategies are producing a diverse and correctly labeled set of examples on real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0d825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk for preview: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\train\\rois_chunk_0008.pt\n",
      "Found 2160 unique images in this chunk. Visualizing 30 of them.\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026064_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026612_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00029486_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00027563_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00025745_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026067_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00029687_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026225_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00029374_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00029378_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026837_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00029579_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026078_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00028048_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00027963_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026032_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00028207_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00028129_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00029511_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00028739_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00025844_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00027713_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026109_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00028196_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00029372_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026409_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00027740_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026876_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00026179_preview.jpg\n",
      "Preview saved: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\preview\\00027918_preview.jpg\n"
     ]
    }
   ],
   "source": [
    "def draw_boxes_pt(image_path: Path, boxes: torch.Tensor, metadata: List[Dict], out_path: Path, max_draw=100):\n",
    "    \"\"\"Draws boxes from a .pt chunk onto an image.\"\"\"\n",
    "    if not image_path.exists():\n",
    "        print(f\"Warning: Image not found at {image_path}, cannot draw preview.\")\n",
    "        return\n",
    "        \n",
    "    with Image.open(image_path).convert(\"RGB\") as im:\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        color_map = {\n",
    "            \"gt_pos\": (128,0,255),  # Purple\n",
    "            \"pred_tp\": (0,255,0),    # Green\n",
    "            \"pred_fp\": (255,0,0),    # Red\n",
    "            \"jitter_pos\": (0,200,255),# Cyan\n",
    "            \"jitter_neg\": (255,128,0),# Orange\n",
    "            \"bg_neg\": (200,200,200),  # Gray\n",
    "        }\n",
    "        for i in range(min(len(boxes), max_draw)):\n",
    "            box = boxes[i].tolist()\n",
    "            meta = metadata[i]\n",
    "            sample_type = meta.get('type', 'unknown')\n",
    "            color = color_map.get(sample_type, (255, 255, 0)) # Yellow for unknown\n",
    "            draw.rectangle(box, outline=color, width=2)\n",
    "    im.save(out_path)\n",
    "    print(f\"Preview saved: {out_path}\")\n",
    "\n",
    "# --- Load a chunk and visualize a few images from it ---\n",
    "PREVIEW_IMAGES = 30\n",
    "chunk_to_load = None\n",
    "if train_chunk_paths:\n",
    "    chunk_to_load = random.choice(train_chunk_paths)\n",
    "elif val_chunk_paths:\n",
    "    chunk_to_load = random.choice(val_chunk_paths)\n",
    "\n",
    "if chunk_to_load:\n",
    "    print(f\"Loading chunk for preview: {chunk_to_load}\")\n",
    "    data = torch.load(chunk_to_load)\n",
    "    \n",
    "    # Group boxes and metadata by image path\n",
    "    rois_by_image = defaultdict(lambda: {'boxes': [], 'metadata': []})\n",
    "    all_boxes = data['boxes']\n",
    "    all_metadata = data['metadata']\n",
    "\n",
    "    for i, meta in enumerate(all_metadata):\n",
    "        img_path = meta.get('img_path')\n",
    "        if img_path:\n",
    "            rois_by_image[img_path]['boxes'].append(all_boxes[i])\n",
    "            rois_by_image[img_path]['metadata'].append(meta)\n",
    "    \n",
    "    # Select a few images from this chunk to visualize\n",
    "    image_paths_in_chunk = list(rois_by_image.keys())\n",
    "    images_to_preview = random.sample(image_paths_in_chunk, k=min(PREVIEW_IMAGES, len(image_paths_in_chunk)))\n",
    "    \n",
    "    print(f\"Found {len(image_paths_in_chunk)} unique images in this chunk. Visualizing {len(images_to_preview)} of them.\")\n",
    "    for img_path_str in images_to_preview:\n",
    "        img_path = Path(img_path_str)\n",
    "        img_data = rois_by_image[img_path_str]\n",
    "        \n",
    "        # Stack the list of tensors into a single tensor for drawing\n",
    "        boxes_tensor = torch.stack(img_data['boxes'])\n",
    "        \n",
    "        out_path = PREVIEW_DIR / (img_path.stem + \"_preview.jpg\")\n",
    "        draw_boxes_pt(img_path, boxes_tensor, img_data['metadata'], out_path)\n",
    "\n",
    "else:\n",
    "    print(\"No chunks found to generate a preview.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97018e",
   "metadata": {},
   "source": [
    "## Dataset Stats & Class Weights (from `.pt` Chunks)\n",
    "\n",
    "This cell calculates summary statistics and class weights for the L2 head. It reads data from our `.pt` chunk format and uses an **adaptive beta** for class-balanced weighting.\n",
    "\n",
    "### Engineering Decision:\n",
    "- **Efficient Iteration**: The `iter_chunk_metadata` function loads each `.pt` file and yields only the `metadata`, which is memory-efficient.\n",
    "- **Adaptive Class-Balanced Weights**: We calculate an adaptive `beta` based on the frequency of the most common class. This is a robust method from the paper \"Class-Balanced Loss Based on Effective Number of Samples\" (Cui et al., 2019).\n",
    "- **Stable Normalization**: To prevent excessively large weights that could destabilize training, we normalize the final weights so that their **mean is 1.0**. This balances the importance of rare classes without causing volatile gradients, making the training process less sensitive to the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d74993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Total ROIs: 678,015\n",
      "[Train] Counts by type: {'gt_pos': 188542, 'pred_tp': 181304, 'pred_fp': 13800, 'jitter_neg': 41554, 'jitter_pos': 77040, 'bg_neg': 175775}\n",
      "[Train] L2 positive counts: {'heavy_vehicle': 32097, 'car_group': 408075, 'two_wheeled_vehicle': 6714}\n",
      "\n",
      "[Val] Total ROIs: 157,400\n",
      "[Val] Counts by type: {'gt_pos': 48226, 'pred_tp': 44110, 'jitter_pos': 19612, 'jitter_neg': 10647, 'bg_neg': 31496, 'pred_fp': 3309}\n",
      "[Val] L2 positive counts: {'heavy_vehicle': 13147, 'car_group': 97594, 'two_wheeled_vehicle': 1207}\n",
      "\n",
      "Adaptive beta calculated based on max class frequency: 0.9999975\n",
      "L2 class weights (normalized to mean=1): {'heavy_vehicle': 0.5213, 'car_group': 0.0624, 'two_wheeled_vehicle': 2.4163}\n",
      "\n",
      "Stats saved to: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\stats_train.json\n",
      "Stats saved to: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\stats_val.json\n",
      "Class weights saved to: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\class_weights.json\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def iter_chunk_metadata(paths: List[str]):\n",
    "    \"\"\"Generator to iterate through metadata from all .pt chunk files.\"\"\"\n",
    "    for cp in paths:\n",
    "        try:\n",
    "            data = torch.load(cp, map_location='cpu')\n",
    "            for meta_item in data['metadata']:\n",
    "                yield meta_item\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load or process chunk {cp}. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "# Calculate stats for the training split\n",
    "counts_by_type_train = defaultdict(int)\n",
    "counts_by_l2_train = defaultdict(int)\n",
    "total_train = 0\n",
    "\n",
    "for meta in iter_chunk_metadata(train_chunk_paths):\n",
    "    counts_by_type_train[meta[\"type\"]] += 1\n",
    "    total_train += 1\n",
    "    if meta.get(\"gt_cls_l2\") is not None:\n",
    "        counts_by_l2_train[meta[\"gt_cls_l2\"]] += 1\n",
    "\n",
    "print(f\"[Train] Total ROIs: {total_train:,}\")\n",
    "print(f\"[Train] Counts by type: {dict(counts_by_type_train)}\")\n",
    "l2_counts_named = {L2_NAMES[k]: v for k, v in sorted(counts_by_l2_train.items())}\n",
    "print(f\"[Train] L2 positive counts: {l2_counts_named}\")\n",
    "\n",
    "# Calculate stats for the validation split\n",
    "total_val = 0\n",
    "if val_chunk_paths:\n",
    "    counts_by_type_val = defaultdict(int)\n",
    "    counts_by_l2_val = defaultdict(int)\n",
    "    for meta in iter_chunk_metadata(val_chunk_paths):\n",
    "        counts_by_type_val[meta[\"type\"]] += 1\n",
    "        total_val += 1\n",
    "        if meta.get(\"gt_cls_l2\") is not None:\n",
    "            counts_by_l2_val[meta[\"gt_cls_l2\"]] += 1\n",
    "\n",
    "    print(f\"\\n[Val] Total ROIs: {total_val:,}\")\n",
    "    print(f\"[Val] Counts by type: {dict(counts_by_type_val)}\")\n",
    "    l2_counts_named_val = {L2_NAMES[k]: v for k, v in sorted(counts_by_l2_val.items())}\n",
    "    print(f\"[Val] L2 positive counts: {l2_counts_named_val}\")\n",
    "else:\n",
    "    print(\"\\n[Val] No validation chunks found.\")\n",
    "\n",
    "# Calculate Class-Balanced Weights for L2 with Adaptive Beta\n",
    "def effective_number_weights(counts: Dict[int, int], beta: float):\n",
    "    \"\"\"Calculates class-balanced weights and normalizes them to have a mean of 1.\"\"\"\n",
    "    weights = {}\n",
    "    num_classes = len(L2_NAMES)\n",
    "    if not counts: return {k: 1.0 for k in range(num_classes)}\n",
    "    \n",
    "    # 1. Calculate raw weights based on effective number of samples\n",
    "    for k in range(num_classes):\n",
    "        n_k = counts.get(k, 1)\n",
    "        weights[k] = (1.0 - beta) / (1.0 - (beta ** n_k))\n",
    "    \n",
    "    # 2. Normalize weights so their mean is 1.0 for training stability\n",
    "    sum_of_weights = sum(weights.values())\n",
    "    mean_weight = sum_of_weights / num_classes\n",
    "    \n",
    "    final_weights = {k: v / mean_weight for k, v in weights.items()}\n",
    "    return final_weights\n",
    "\n",
    "# Derive adaptive beta from train positives\n",
    "if counts_by_l2_train:\n",
    "    n_max = max(1, max(counts_by_l2_train.values()))\n",
    "    beta_used = 1.0 - 1.0 / n_max\n",
    "    beta_used = float(np.clip(beta_used, 0.9, 0.9999999)) # Clamp for stability\n",
    "else:\n",
    "    beta_used = 0.999 # Fallback if no positive samples\n",
    "\n",
    "print(f\"\\nAdaptive beta calculated based on max class frequency: {beta_used:.7f}\")\n",
    "\n",
    "l2_weights = effective_number_weights(counts_by_l2_train, beta=beta_used)\n",
    "print(\"L2 class weights (normalized to mean=1):\", {L2_NAMES[k]: round(v, 4) for k, v in l2_weights.items()})\n",
    "\n",
    "# Save Stats and Weights\n",
    "with open(STATS_TRAIN_PATH, \"w\") as f:\n",
    "    json.dump({'total_roi': total_train, 'counts_by_type': dict(counts_by_type_train), 'l2_positive_counts': dict(counts_by_l2_train)}, f, indent=2)\n",
    "if val_chunk_paths:\n",
    "    with open(STATS_VAL_PATH, \"w\") as f:\n",
    "        json.dump({'total_roi': total_val, 'counts_by_type': dict(counts_by_type_val), 'l2_positive_counts': dict(counts_by_l2_val)}, f, indent=2)\n",
    "\n",
    "with open(CLASS_WEIGHTS_PATH, \"w\") as f:\n",
    "    json.dump({\n",
    "        'l2_weights': l2_weights,\n",
    "        'l2_beta_used': beta_used,\n",
    "        'l2_names': L2_NAMES,\n",
    "        'l1_names': L1_NAMES,\n",
    "        'l3_names': L3_NAMES\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\nStats saved to:\", STATS_TRAIN_PATH)\n",
    "if val_chunk_paths: print(\"Stats saved to:\", STATS_VAL_PATH)\n",
    "print(\"Class weights saved to:\", CLASS_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e17ed9",
   "metadata": {},
   "source": [
    "## Write Manifest File\n",
    "\n",
    "This final step creates the `manifest.json` file. This file is the bridge between our data preparation and the training script. It contains all the necessary information for the trainer to understand the dataset, including paths to the `.pt` chunks and the configuration parameters used to generate the features.\n",
    "\n",
    "### Engineering Decision:\n",
    "- **Self-Contained Configuration**: By including the `feature_extraction` parameters (`COMPRESSED_DIM`, etc.) in the manifest, we make the training process more robust. The training script can read these values and dynamically build a compatible model, preventing errors from mismatched feature dimensions.\n",
    "- **Portability**: All paths are saved as relative paths from the project root. This ensures that the entire project folder can be moved to a different machine without breaking the file links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50735460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest written to: c:\\Users\\Mika\\Desktop\\New_Training_Run_Post_Bulgaria\\aux_heads_chunks_pt\\manifest.json\n",
      "\n",
      "✅ Data preparation notebook is complete!\n",
      "You are now ready to run the training script for the auxiliary heads.\n"
     ]
    }
   ],
   "source": [
    "manifest = {\n",
    "    \"chunks\": {\n",
    "        \"train\": [str(Path(p).relative_to(ROOT)) for p in train_chunk_paths],\n",
    "        \"val\": [str(Path(p).relative_to(ROOT)) for p in val_chunk_paths]\n",
    "    },\n",
    "    \"hierarchy\": {\n",
    "        \"L1_NAMES\": L1_NAMES,\n",
    "        \"L2_NAMES\": L2_NAMES,\n",
    "        \"L3_NAMES\": L3_NAMES,\n",
    "        \"CLASS_TO_L2\": CLASS_TO_L2\n",
    "    },\n",
    "    \"feature_extraction\": { # <-- NEW SECTION\n",
    "        \"roi_align_size\": ROI_ALIGN_SIZE,\n",
    "        \"compressed_dim\": COMPRESSED_DIM,\n",
    "        \"pyramid_thresholds\": PYRAMID_THRESHOLDS\n",
    "    },\n",
    "    \"thresholds\": {\n",
    "        \"IOU_POS\": IOU_POS,\n",
    "        \"IOU_NEG\": IOU_NEG,\n",
    "        \"IOU_BG\":  IOU_BG,\n",
    "    },\n",
    "    \"sampling\": {\n",
    "        \"JITTER_POS_PER_GT\": JITTER_POS_PER_GT,\n",
    "        \"JITTER_NEG_PER_GT\": JITTER_NEG_PER_GT,\n",
    "        \"BG_NEG_PER_IMG\": BG_NEG_PER_IMG\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"data_yaml\": str(DATA_YAML.relative_to(ROOT)),\n",
    "        \"l3_checkpoint\": str(L3_CHECKPOINT.relative_to(ROOT)),\n",
    "        \"out_dir\": str(OUT_DIR.relative_to(ROOT)),\n",
    "        \"stats_train\": str(STATS_TRAIN_PATH.relative_to(ROOT)),\n",
    "        \"stats_val\": str(STATS_VAL_PATH.relative_to(ROOT)) if val_chunk_paths else None,\n",
    "        \"class_weights\": str(CLASS_WEIGHTS_PATH.relative_to(ROOT)),\n",
    "    },\n",
    "    \"seed\": SEED\n",
    "}\n",
    "\n",
    "with open(MANIFEST_PATH, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(\"Manifest written to:\", MANIFEST_PATH)\n",
    "print(\"\\n✅ Data preparation notebook is complete!\")\n",
    "print(\"You are now ready to run the training script for the auxiliary heads.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hier-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
